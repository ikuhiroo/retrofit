{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# python3\n",
    "#\n",
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import numpy\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isNumber = re.compile(r'\\d+.*')\n",
    "def norm_word(word):\n",
    "    if isNumber.search(word.lower()):\n",
    "        return '---num---'\n",
    "    elif re.sub(r'\\W+', '', word) == '':\n",
    "        return '---punc---'\n",
    "    else:\n",
    "        return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read all the word vectors and normalize them\"\"\"\n",
    "def read_word_vecs(filename):\n",
    "    wordVectors = {}\n",
    "    # ファイル読み込み\n",
    "    if filename.endswith('.gz'): \n",
    "        fileObject = gzip.open(filename, 'r')\n",
    "    else: \n",
    "        fileObject = codecs.open(filename, \"r\", \"utf-8\", 'ignore')\n",
    "        \n",
    "    for line in fileObject:\n",
    "        # line = line.strip().lower()\n",
    "        line = line.strip()\n",
    "        word = line.split()[0]\n",
    "        wordVectors[word] = numpy.zeros(len(line.split())-1, dtype=float)\n",
    "        for index, vecVal in enumerate(line.split()[1:]):\n",
    "            wordVectors[word][index] = float(vecVal)\n",
    "        \"\"\"normalize weight vector\"\"\"\n",
    "        wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "\n",
    "    sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "    return wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Write word vectors to file\"\"\"\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "    sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "    outFile = open(outFileName, 'w')  \n",
    "    for word, values in wordVectors.items():\n",
    "        outFile.write(word+' ')\n",
    "        for val in wordVectors[word]:\n",
    "            outFile.write('%.4f' %(val)+' ')\n",
    "        outFile.write('\\n')      \n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read the PPDB.etc word relations as a dictionary\"\"\"\n",
    "def read_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    fileObject = open(filename, 'r')\n",
    "    for line in fileObject:\n",
    "        words = line.lower().strip().split()\n",
    "        lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Retrofit word vectors to a lexicon\"\"\"\n",
    "def retrofit(wordVecs, lexicon, numIters):\n",
    "    # Input word vecs\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    # Input word vecsの単語リスト\n",
    "    wvVocab = set(newWordVecs.keys())\n",
    "    # wvVocabとlexiconの共通単語\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "\n",
    "    for _ in range(numIters):\n",
    "        # loop through every node also in ontology (else just use data estimate)\n",
    "        for word in loopVocab:\n",
    "            # lexicon wordの近傍単語とwvVocabの共通単語とその個数\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "            # no neighbours -> pass - use data estimate\n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "            \"\"\"分散表現の更新手続き\"\"\"\n",
    "            # the weight of the data estimate if the number of neighbours\n",
    "            newVec = numNeighbours * wordVecs[word]\n",
    "            # loop over neighbours and add to new vector (currently with weight 1)\n",
    "            for ppWord in wordNeighbours:\n",
    "                newVec += newWordVecs[ppWord]\n",
    "            newWordVecs[word] = newVec/(2*numNeighbours)\n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / n1 / n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input word vecs -> original\n",
    "input_arg = './word2vec/vectors.model'\n",
    "# Lexicon file name\n",
    "lexicon_arg = './lexicons/wordnet-jpn.txt'\n",
    "# Num iterations\n",
    "numiter_arg = 10\n",
    "# Output word vecs -> retrofitting\n",
    "output_arg = './out_vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIter = int(numiter_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outFileName = output_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon = read_lexicon(lexicon_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ./word2vec/vectors.model \n"
     ]
    }
   ],
   "source": [
    "wordVecs = read_word_vecs(input_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = retrofit(wordVecs, lexicon, numIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrofittingした分散表現を保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing down the vectors in ./out_vec.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Enrich the word vectors using ppdb and print the enriched vectors\"\"\"\n",
    "print_word_vecs(new_vec, outFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = 'トラ'\n",
    "# path = \"./fastText/model.vec\"\n",
    "negative = False # Falseなら似た単語を候補で上げる\n",
    "threshold = 0.6 # -1なら閾値固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = wordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2 not found error in dict\n",
      "0.8562229551754349\n"
     ]
    }
   ],
   "source": [
    "# 特定単語の類似度算出\n",
    "v1 = 'アテネ'\n",
    "v2 = 'ギリシャ'\n",
    "\n",
    "if v1 not in lexicon:\n",
    "    print(\"v1 not found error in dict\")\n",
    "if v2 not in lexicon:\n",
    "    print(\"v2 not found error in dict\")\n",
    "    \n",
    "try:\n",
    "    print(similarity(vecs[v1], vecs[v2]))\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs_wordVecs = wordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs_new_vec = new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec : 0.48468072664976913\n",
      "retrofitting : 0.4639886453578066\n"
     ]
    }
   ],
   "source": [
    "# アナロジー算出\n",
    "v1 = 'オタワ'\n",
    "v2 = 'カナダ'\n",
    "v3 = 'ストックホルム'\n",
    "v4 = 'スウェーデン'\n",
    "\n",
    "if v1 not in lexicon:\n",
    "    print(\"v1 not found error in dict\")\n",
    "if v2 not in lexicon:\n",
    "    print(\"v2 not found error in dict\")\n",
    "if v3 not in lexicon:\n",
    "    print(\"v3 not found error in dict\")\n",
    "if v4 not in lexicon:\n",
    "    print(\"v4 not found error in dict\")\n",
    "    \n",
    "try:\n",
    "    print('word2vec : {}'.format(similarity(vecs_wordVecs[v1] + vecs_wordVecs[v2], vecs_wordVecs[v3] + vecs_wordVecs[v4])))\n",
    "except:\n",
    "    print('error')\n",
    "\n",
    "try:\n",
    "    print('retrofitting : {}'.format(similarity(vecs_new_vec[v1] + vecs_new_vec[v2], vecs_new_vec[v3] + vecs_new_vec[v4])))\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wordの設定確認\n",
    "if not word:\n",
    "    raise Exception(\"word is missing\")\n",
    "    \n",
    "# wordがモデルにない場合，\n",
    "if word not in vecs:\n",
    "    raise Exception(\"Sorry, this word is not registered in model.\")\n",
    "w_vec = vecs[word]\n",
    "\n",
    "# ナレッジグラフにあるかどうかの確認\n",
    "lexicon = read_lexicon(lexicon_arg)\n",
    "if word not in lexicon:\n",
    "#     raise Exception(\"not found error in dict\")\n",
    "    print(\"not found error in dict\")\n",
    "\n",
    "# 閾値の設定\n",
    "border_positive = threshold if threshold > 0 else 0.8\n",
    "border_negative = threshold if threshold > 0 else 0.3\n",
    "\n",
    "# 候補数の設定\n",
    "max_candidates = 20\n",
    "candidates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754069 is not valid word.\n"
     ]
    }
   ],
   "source": [
    "for w in vecs:\n",
    "    try:\n",
    "        if w_vec.shape != vecs[w].shape:\n",
    "            raise Exception(\"size not match\")\n",
    "        s = similarity(w_vec, vecs[w])\n",
    "    except Exception as ex:\n",
    "        print(w + \" is not valid word.\")\n",
    "        continue\n",
    "\n",
    "    if negative and s <= border_negative:\n",
    "        candidates[w] = s\n",
    "        if len(candidates) % 5 == 0:\n",
    "            border_negative -= 0.05\n",
    "    elif not negative and s >= border_positive:\n",
    "        candidates[w] = s\n",
    "        if len(candidates) % 5 == 0:\n",
    "            border_positive += 0.05\n",
    "\n",
    "    if len(candidates) > max_candidates:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トラ, 1.0000000000000002\n",
      "虎, 0.7187464339674907\n",
      "タイガー, 0.650029262972028\n"
     ]
    }
   ],
   "source": [
    "# 類義語算出\n",
    "sorted_candidates = sorted(candidates, key=candidates.get, reverse=not negative)\n",
    "for c in sorted_candidates:\n",
    "    print(\"{0}, {1}\".format(c, candidates[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
