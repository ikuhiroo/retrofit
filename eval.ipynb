{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordSim評価\n",
    "# wordAnalogy評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------初期設定-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# python3\n",
    "#\n",
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import numpy\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isNumber = re.compile(r'\\d+.*')\n",
    "def norm_word(word):\n",
    "    if isNumber.search(word.lower()):\n",
    "        return '---num---'\n",
    "    elif re.sub(r'\\W+', '', word) == '':\n",
    "        return '---punc---'\n",
    "    else:\n",
    "        return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read all the word vectors and normalize them\"\"\"\n",
    "def read_word_vecs(filename):\n",
    "    wordVectors = {}\n",
    "    # ファイル読み込み\n",
    "    if filename.endswith('.gz'): \n",
    "        fileObject = gzip.open(filename, 'r')\n",
    "    else: \n",
    "        fileObject = codecs.open(filename, \"r\", \"utf-8\", 'ignore')\n",
    "        \n",
    "    for line in fileObject:\n",
    "        # line = line.strip().lower()\n",
    "        line = line.strip()\n",
    "        word = line.split()[0]\n",
    "        wordVectors[word] = numpy.zeros(len(line.split())-1, dtype=float)\n",
    "        for index, vecVal in enumerate(line.split()[1:]):\n",
    "            wordVectors[word][index] = float(vecVal)\n",
    "        \"\"\"normalize weight vector\"\"\"\n",
    "        wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "\n",
    "    sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "    return wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Write word vectors to file\"\"\"\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "    sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "    outFile = open(outFileName, 'w')  \n",
    "    for word, values in wordVectors.items():\n",
    "        outFile.write(word+' ')\n",
    "        for val in wordVectors[word]:\n",
    "            outFile.write('%.4f' %(val)+' ')\n",
    "        outFile.write('\\n')      \n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read the PPDB.etc word relations as a dictionary\"\"\"\n",
    "def read_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    fileObject = open(filename, 'r')\n",
    "    for line in fileObject:\n",
    "        words = line.lower().strip().split()\n",
    "        lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Retrofit word vectors to a lexicon\"\"\"\n",
    "def retrofit(wordVecs, lexicon, numIters):\n",
    "    # Input word vecs\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    # Input word vecsの単語リスト\n",
    "    wvVocab = set(newWordVecs.keys())\n",
    "    # wvVocabとlexiconの共通単語\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "\n",
    "    for _ in range(numIters): #10回程度\n",
    "        # loop through every node also in ontology (else just use data estimate)\n",
    "        for word in loopVocab:\n",
    "            # lexicon wordの近傍単語とwvVocabの共通単語とその個数\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "            # no neighbours -> pass - use data estimate\n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "            \"\"\"分散表現の更新手続き\"\"\"\n",
    "            # the weight of the data estimate if the number of neighbours\n",
    "            newVec = numNeighbours * wordVecs[word]\n",
    "            # loop over neighbours and add to new vector (currently with weight 1)\n",
    "            for ppWord in wordNeighbours:\n",
    "                newVec += newWordVecs[ppWord]\n",
    "            newWordVecs[word] = newVec/(2*numNeighbours)\n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1) # v1のノルム\n",
    "    n2 = np.linalg.norm(v2) # v2のノルム\n",
    "    return np.dot(v1, v2) / (n1*n2) # 内積 / "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・パス指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# オリジナルの分散表現\n",
    "input_arg = './word2vec/vectors.model'\n",
    "# wordnetの辞書\n",
    "lexicon_arg = './result.txt'\n",
    "# retrofittingしたnewvec\n",
    "output_arg = './out_vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Num iterations\n",
    "numiter_arg = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・初期vecとnewvecとwordsim辞書のread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIter = int(numiter_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outFileName = output_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon = read_lexicon(lexicon_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ./word2vec/vectors.model \n"
     ]
    }
   ],
   "source": [
    "wordVecs = read_word_vecs(input_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ./out_vec.txt \n"
     ]
    }
   ],
   "source": [
    "new_vec = read_word_vecs(output_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------WordSim評価-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "vecs_wordVecs = wordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrofitting\n",
    "vecs_new_vec = new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checkSim(v1, v2):\n",
    "    if v1 not in lexicon: # 注目単語がwordnetに含まれない場合，\n",
    "        print(\"v1(={})はwordnetのkeyに存在しません\".format(v1))\n",
    "        wordNeighbours_1 = set()\n",
    "    else:\n",
    "        # 注目単語の同義語リストとword2vecのkeyリストと重複する単語リスト（更新対象か？）\n",
    "        wordNeighbours_1 = set(lexicon[v1]).intersection(set(wordVecs.keys()))\n",
    "        print('v1(={})における更新対象のneighbour数 : {}'.format(v1, len(wordNeighbours_1)))\n",
    "        \n",
    "    # neighboursの中で注目単語と類似度が低いもの\n",
    "    ave_1_word2vec = np.zeros_like(wordVecs[v1])  #neighboursの平均ベクトル\n",
    "    ave_1_retrofit = np.zeros_like(wordVecs[v1])\n",
    "    for neighbour in wordNeighbours_1:\n",
    "        print('v1とneighbour(={}) : {} -> {}'.format(neighbour, \n",
    "                                                                   similarity(wordVecs[neighbour], wordVecs[v1]), \n",
    "                                                                   similarity(vecs_new_vec[neighbour], vecs_new_vec[v1])))\n",
    "        ave_1_word2vec += wordVecs[neighbour]\n",
    "        ave_1_retrofit += vecs_new_vec[neighbour]\n",
    "    print('v1とneighboursの平均ベクトル : {} -> {}'.format(similarity(ave_1_word2vec/len(wordNeighbours_1), wordVecs[v1]), \n",
    "                                                              similarity(ave_1_retrofit/len(wordNeighbours_1), vecs_new_vec[v1])))\n",
    "    print(\" \")\n",
    "    \n",
    "    if v2 not in lexicon:\n",
    "        print(\"v2(={})はwordnetのkeyに存在しません\".format(v2))\n",
    "        wordNeighbours_2 = set()\n",
    "    else:\n",
    "        wordNeighbours_2 = set(lexicon[v2]).intersection(set(wordVecs.keys()))\n",
    "        print('v2(={})における更新対象のneighbour数 : {}'.format(v2, len(wordNeighbours_2))) # 更新対象数\n",
    "    \n",
    "    # neighboursの中で注目単語と類似度が低いもの\n",
    "    ave_2_word2vec = np.zeros_like(wordVecs[v2])  #neighboursの平均ベクトル\n",
    "    ave_2_retrofit = np.zeros_like(wordVecs[v2])\n",
    "    for neighbour in wordNeighbours_2:\n",
    "        print('v2とneighbour(={}) : {} -> {}'.format(neighbour, \n",
    "                                                                   similarity(wordVecs[neighbour], wordVecs[v2]), \n",
    "                                                                   similarity(vecs_new_vec[neighbour], vecs_new_vec[v2])))\n",
    "        ave_2_word2vec += wordVecs[neighbour]\n",
    "        ave_2_retrofit += vecs_new_vec[neighbour]\n",
    "    print('v2とneighboursの平均ベクトル : {} -> {}'.format(similarity(ave_2_word2vec/len(wordNeighbours_2), wordVecs[v2]), \n",
    "                                                              similarity(ave_2_retrofit/len(wordNeighbours_2), vecs_new_vec[v2])))\n",
    "    print(\" \")\n",
    "    \n",
    "    # v1とv2における更新対象のneighbourに重複があるか\n",
    "    print('更新対象における重複数 : {}'.format(len(set(wordNeighbours_1).intersection(set(wordNeighbours_2)))))\n",
    "    print(\" \")\n",
    "\n",
    "    \"\"\"v1とv2のword2vecとretrofittingにおける類似度\"\"\"\n",
    "    try:\n",
    "        print(\"word2vec : {}\".format(similarity(vecs_wordVecs[v1], vecs_wordVecs[v2])))\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "    try:\n",
    "        print(\"retrofitting : {}\".format(similarity(vecs_new_vec[v1], vecs_new_vec[v2])))\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "    print(\" \")\n",
    "    \"\"\"同一単語におけるword2vecとretrofittingの類似度\"\"\"\n",
    "    try:\n",
    "        print(\"v1 : {}\".format(similarity(vecs_wordVecs[v1], vecs_new_vec[v1])))\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "    try:\n",
    "        print(\"v2 : {}\".format(similarity(vecs_wordVecs[v2], vecs_new_vec[v2])))\n",
    "    except:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・WordSimのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1(=フットボール)における更新対象のneighbour数 : 4\n",
      "v1とneighbour(=サッカー) : 0.6700097012259578 -> 0.9023682559373559\n",
      "v1とneighbour(=フートボール) : 0.41945202281722355 -> 0.8277756438161145\n",
      "v1とneighbour(=蹴球) : 0.4714661654042284 -> 0.8447562314376742\n",
      "v1とneighbour(=アソシエーションフットボール) : 0.44592722294673065 -> 0.8335533028545058\n",
      "v1とneighboursの平均ベクトル : 0.6533150118662724 -> 0.9099880161508288\n",
      " \n",
      "v2(=サッカー)における更新対象のneighbour数 : 4\n",
      "v2とneighbour(=フートボール) : 0.3560841190233665 -> 0.8074251482783557\n",
      "v2とneighbour(=フットボール) : 0.6700097012259578 -> 0.9023682559373559\n",
      "v2とneighbour(=蹴球) : 0.49144473934055627 -> 0.8494646429952011\n",
      "v2とneighbour(=アソシエーションフットボール) : 0.3838424344300787 -> 0.8132187145866909\n",
      "v2とneighboursの平均ベクトル : 0.6121738109726407 -> 0.8980829186067604\n",
      " \n",
      "更新対象における重複数 : 3\n",
      " \n",
      "word2vec : 0.6700097012259578\n",
      "retrofitting : 0.9023682559373559\n",
      " \n",
      "v1 : 0.9490976274917442\n",
      "v2 : 0.9421475806597656\n"
     ]
    }
   ],
   "source": [
    "v1 = 'フットボール'\n",
    "v2 = 'サッカー'\n",
    "checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = 'ドル'\n",
    "v2 = '円'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = '生'\n",
    "v2 = '死'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = '美術館'\n",
    "v2 = '劇場'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = '王'\n",
    "v2 = '城'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = 'テレビ'\n",
    "v2 = 'ラジオ'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = '飲む'\n",
    "v2 = '食べる'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = '医師'\n",
    "v2 = '看護師'\n",
    "# checkSim(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------wordAnalogy評価-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkAnalogy(vecs, w_vec):  \n",
    "    negative = False # Falseなら似た単語を候補で上げる\n",
    "    threshold = 0.3 # -1なら閾値固定\n",
    "\n",
    "    # 閾値の設定\n",
    "    border_positive = threshold if threshold > 0 else 0.9\n",
    "    border_negative = threshold if threshold > 0 else 0.2\n",
    "    print('{} < thd < {}'.format(border_negative, border_positive))\n",
    "\n",
    "    # 候補数の設定\n",
    "    max_candidates = 20\n",
    "    candidates = {}\n",
    "\n",
    "    for w in vecs:\n",
    "        try:\n",
    "            if w_vec.shape != vecs[w].shape:\n",
    "                raise Exception(\"size not match\")\n",
    "            s = similarity(w_vec, vecs[w])\n",
    "        except Exception as ex:\n",
    "            print(w + \" is not valid word.\")\n",
    "            continue\n",
    "\n",
    "        if negative and s <= border_negative:\n",
    "            candidates[w] = s\n",
    "            if len(candidates) % 5 == 0:\n",
    "                border_negative -= 0.05\n",
    "        elif not negative and s >= border_positive:\n",
    "            candidates[w] = s\n",
    "            if len(candidates) % 5 == 0:\n",
    "                border_positive += 0.05\n",
    "\n",
    "        if len(candidates) > max_candidates:\n",
    "            break\n",
    "\n",
    "    # 類義語算出\n",
    "    sorted_candidates = sorted(candidates, key=candidates.get, reverse=not negative)\n",
    "    for c in sorted_candidates:\n",
    "        print(\"{0}, {1}\".format(c, candidates[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・「v1 + v2 - v3」と「v4」の類似度算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = '兄'\n",
    "v2 = '姉'\n",
    "v3 = '祖父'\n",
    "v4 = '祖母'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec : 0.3855693677632116\n",
      "retrofitting : 0.46470763443894203\n"
     ]
    }
   ],
   "source": [
    "if v1 not in lexicon:\n",
    "    print(\"v1 not found error in dict\")\n",
    "if v2 not in lexicon:\n",
    "    print(\"v2 not found error in dict\")\n",
    "if v3 not in lexicon:\n",
    "    print(\"v3 not found error in dict\")\n",
    "if v4 not in lexicon:\n",
    "    print(\"v4 not found error in dict\")\n",
    "\n",
    "try:\n",
    "    print('word2vec : {}'.format(similarity(vecs_wordVecs[v1] + vecs_wordVecs[v2] - vecs_wordVecs[v3], vecs_wordVecs[v4])))\n",
    "except:\n",
    "    print('error')\n",
    "\n",
    "try:\n",
    "    print('retrofitting : {}'.format(similarity(vecs_new_vec[v1] + vecs_new_vec[v2] - vecs_new_vec[v3], vecs_new_vec[v4])))\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・「v1 + v2 - v3」と近い単語を挙げる→「v4」が結果に出るか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 < thd < 0.3\n",
      "754069 is not valid word.\n",
      "姉, 0.8433279735101462\n",
      "兄, 0.7622779357580127\n",
      "妹, 0.7553375375315672\n",
      "弟, 0.7011033258786433\n",
      "双子, 0.5541898022518956\n",
      "夫, 0.5241162973051148\n",
      "年上, 0.5228285283574702\n",
      "次女, 0.49053484292919564\n",
      "兄弟, 0.4801388417438813\n",
      "叔父, 0.48004084731816965\n",
      "甥, 0.4646477248521072\n",
      "母, 0.4618720699656536\n",
      "長女, 0.4522498603989746\n",
      "娘。, 0.4484254297338895\n",
      "彼女, 0.41874909255787995\n",
      "妻, 0.4180556386599047\n",
      "息子, 0.38730153006548584\n",
      "父, 0.3860297212008132\n",
      "結婚, 0.3574390676533354\n",
      "男, 0.35215392646403093\n",
      "2人, 0.32434570907744076\n",
      " \n",
      "0.3 < thd < 0.3\n",
      "754069 is not valid word.\n",
      "弟, 0.735540018182597\n",
      "彼女, 0.5708677571388876\n",
      "母, 0.543018985376124\n",
      "息子, 0.5335641681270236\n",
      "妻, 0.5269046411495635\n",
      "娘。, 0.5115487458496082\n",
      "女性, 0.4747866974855413\n",
      "子供, 0.46754522886759947\n",
      "彼, 0.464979913115947\n",
      "父, 0.4579658210443133\n",
      "結婚, 0.4532385554739422\n",
      "男性, 0.4523917286989586\n",
      "子, 0.4158272591171027\n",
      "2人, 0.41108535260447876\n",
      "自分, 0.3953707483145449\n",
      "思う, 0.376416952702512\n",
      "出演, 0.36485803889961343\n",
      "メンバー, 0.35210435795316825\n",
      "後, 0.3356467345493145\n",
      "活動, 0.3030128526155883\n",
      "務める, 0.30248141491925656\n"
     ]
    }
   ],
   "source": [
    "# 初期vecの場合，\n",
    "checkAnalogy(vecs_wordVecs, vecs_wordVecs[v1] + vecs_wordVecs[v2] - vecs_wordVecs[v3])\n",
    "print(' ')\n",
    "# newvecの場合，\n",
    "checkAnalogy(vecs_new_vec, vecs_new_vec[v1] + vecs_new_vec[v2] - vecs_new_vec[v3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------ある単語の類似する単語を挙げる-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkSim_by_word(vecs, word):\n",
    "    # 閾値の設定\n",
    "    negative = False # Falseなら似た単語を候補で上げる\n",
    "    threshold = 0.3 # -1なら閾値固定\n",
    "    border_positive = threshold if threshold > 0 else 0.8\n",
    "    border_negative = threshold if threshold > 0 else 0.3\n",
    "\n",
    "    # 候補数の設定\n",
    "    max_candidates = 20\n",
    "    candidates = {}\n",
    "    \n",
    "    # wordの設定確認\n",
    "    if not word:\n",
    "        raise Exception(\"word is missing\")\n",
    "\n",
    "    # wordがモデルにない場合，\n",
    "    if word not in vecs:\n",
    "        raise Exception(\"Sorry, this word is not registered in model.\")\n",
    "\n",
    "    # ベクトルの設定\n",
    "    w_vec = vecs[word]\n",
    "\n",
    "    # ナレッジグラフにあるかどうかの確認\n",
    "    lexicon = read_lexicon(lexicon_arg)\n",
    "    if word not in lexicon:\n",
    "    #     raise Exception(\"not found error in dict\")\n",
    "        print(\"not found error in dict\")\n",
    "    \n",
    "    for w in vecs:\n",
    "        try:\n",
    "            if w_vec.shape != vecs[w].shape:\n",
    "                raise Exception(\"size not match\")\n",
    "            s = similarity(w_vec, vecs[w])\n",
    "        except Exception as ex:\n",
    "            print(w + \" is not valid word.\")\n",
    "            continue\n",
    "\n",
    "        if negative and s <= border_negative:\n",
    "            candidates[w] = s\n",
    "            if len(candidates) % 5 == 0:\n",
    "                border_negative -= 0.05\n",
    "        elif not negative and s >= border_positive:\n",
    "            candidates[w] = s\n",
    "            if len(candidates) % 5 == 0:\n",
    "                border_positive += 0.05\n",
    "\n",
    "        if len(candidates) > max_candidates:\n",
    "            break\n",
    "    \n",
    "    # 類義語算出\n",
    "    sorted_candidates = sorted(candidates, key=candidates.get, reverse=not negative)\n",
    "    for c in sorted_candidates:\n",
    "        print(\"{0}, {1}\".format(c, candidates[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = '彼女'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754069 is not valid word.\n",
      "彼女, 1.0\n",
      "彼, 0.7881711200018091\n",
      "自分, 0.670073388263443\n",
      "彼ら, 0.640895342831867\n",
      "自身, 0.5246801791121426\n",
      "女性, 0.5234110524560043\n",
      "自ら, 0.5182341702311083\n",
      "妻, 0.5180212881638562\n",
      "たち, 0.5033317418848571\n",
      "その, 0.45799455304278275\n",
      "う, 0.4364369398637639\n",
      "そして, 0.4343208537947781\n",
      "それ, 0.38248465445508495\n",
      "人, 0.3587975823461335\n",
      "た, 0.35252306602737504\n",
      "という, 0.351366702926194\n",
      "この, 0.3510216224354535\n",
      "に, 0.33693028236821\n",
      "の, 0.3280942013100719\n",
      "を, 0.3240346814325094\n",
      "、, 0.3091203720976004\n",
      " \n",
      "754069 is not valid word.\n",
      "彼, 0.864642855479408\n",
      "自身, 0.5674859399826648\n",
      "知る, 0.4617902743271744\n",
      "人, 0.4615519253815723\n",
      "それ, 0.4602458636767502\n",
      "家, 0.4544736289164974\n",
      "者, 0.4413520603902445\n",
      "その, 0.4247001308463406\n",
      "いる, 0.41938810278738964\n",
      "こと, 0.41417631043315545\n",
      "中, 0.40988732720672916\n",
      "後, 0.40122504221479827\n",
      "また, 0.4009454755117826\n",
      "ある, 0.3994005874913116\n",
      "なる, 0.3741953852747088\n",
      "た, 0.3673450095585146\n",
      "する, 0.3590333640691226\n",
      "に, 0.34534366653098586\n",
      "の, 0.3441150380679108\n",
      "、, 0.33847056209787146\n",
      "は, 0.31613429952648975\n"
     ]
    }
   ],
   "source": [
    "# 初期vecの場合，\n",
    "checkSim_by_word(vecs_wordVecs, word)\n",
    "print(' ')\n",
    "# newvecの場合，\n",
    "checkSim_by_word(vecs_new_vec, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
